# ğŸŒ¾ Agricultural Crop Production & Yield Optimization Analytics System

## Enterprise-Grade Data Engineering & Analytics Platform for Agriculture

![License](https://img.shields.io/badge/License-MIT-green)
![Python](https://img.shields.io/badge/Python-3.8+-blue)
![Databricks](https://img.shields.io/badge/Databricks-Enabled-red)
![Apache Airflow](https://img.shields.io/badge/Airflow-2.x-blue)
![Delta Lake](https://img.shields.io/badge/Delta%20Lake-ACID-brightgreen)
![Power BI](https://img.shields.io/badge/Power%20BI-Enabled-yellow)

Building a **modern agricultural analytics platform** using a **lakehouse architecture** to transform raw crop data into actionable yield insights.

**Features â€¢ Architecture â€¢ Data Layers â€¢ Analytics**

---

## ğŸ¯ Executive Summary

Agricultural departments and agribusiness organizations collect large volumes of crop production data across **states, districts, seasons, and years**. However, decision-making is often impacted by fragmented datasets, manual analysis, and limited visibility into yield performance.

This project delivers a **production-ready agricultural analytics system** that enables:

- Automated ETL pipelines for crop datasets  
- Standardized and validated yield analytics  
- Identification of high- and low-performing regions  
- Executive-ready dashboards for data-driven planning  

### Key Capabilities
- **Automated ETL Pipeline** â€“ Orchestrated using Apache Airflow  
- **Lakehouse Architecture** â€“ Bronze, Silver, and Gold layers using Delta Lake  
- **Scalable Analytics** â€“ Distributed processing with Databricks & PySpark  
- **Interactive BI** â€“ Power BI dashboards for agricultural insights  

---

## ğŸ“Š Project Impact

| Metric | Value |
|------|------|
| Data Processing Speed | 8â€“10x faster than manual analysis |
| Pipeline Reliability | 99.9% uptime with automated retries |
| Analytics Latency | Near real-time insights |
| Scalability | Handles multi-year, multi-region crop data |

---

## âœ¨ Key Features

### ğŸ—ï¸ Enterprise Architecture
- Medallion lakehouse architecture (Bronzeâ€“Silverâ€“Gold)
- ACID-compliant Delta Lake tables
- Schema enforcement and evolution
- Incremental and idempotent processing

### ğŸ”„ Workflow Automation
- Apache Airflow DAG orchestration
- Dependency management between layers
- Automated retries and failure alerts
- Centralized execution logging

### ğŸ“ˆ Advanced Analytics
- Crop-wise production trend analysis
- Yield comparison across states and districts
- Seasonal and yearly performance evaluation
- KPI-driven aggregations for BI

### ğŸ“Š Business Intelligence
- Interactive Power BI dashboards
- Drill-down analysis by region and year
- KPI cards for production and yield
- Filter-driven insights for stakeholders

---

## ğŸ›ï¸ Architecture

### System Architecture Overview

<p align="center">
  <img src="images/system_architecture.png" width="850"/>
  <br>
  <em>
    End-to-End Agricultural Crop Production & Yield Optimization Analytics Architecture
  </em>
</p>

This system follows a modern **lakehouse-based architecture** orchestrated by Apache Airflow, 
designed to process large-scale agricultural datasets and deliver analytics-ready insights.

**Architecture Flow:**
- Multiple agricultural data sources (Crop Production, Rainfall, Soil Health, Fertilizer Usage)
- Python & Pandas-based ingestion layer
- Delta Lakeâ€“backed Bronze, Silver, and Gold layers
- PySpark transformations for scalable analytics
- Power BI dashboards for visualization and decision support

---
## ğŸ§° Technology Stack

### Core Technologies

| Category | Technology | Purpose |
|-------|-----------|--------|
| Processing | PySpark | Distributed data processing |
| Platform | Databricks | Unified analytics workspace |
| Storage | Delta Lake | ACID-compliant lakehouse |
| Orchestration | Apache Airflow | Workflow automation |
| Visualization | Power BI | Interactive dashboards |
| DevOps | Docker | Environment management |

### Infrastructure Components
- Python 3.8+ for ETL scripting
- Docker & Docker Compose for Airflow
- Git & GitHub for version control
- Databricks Notebooks for development

---
## ğŸ“ Project Structure

The project follows an enterprise-grade data engineering layout with clear separation of orchestration, data sources, processing, analytics, and visualization layers.

```text
Agricultural-Crop-Yield-Analytics/
â”‚
â”œâ”€â”€ capstone_airflow/                 # Workflow Orchestration (Apache Airflow)
â”‚   â”œâ”€â”€ airflow-dags/                 # Airflow DAG definitions
â”‚   â”œâ”€â”€ airflow-logs/                 # Pipeline execution & audit logs
â”‚   â”œâ”€â”€ airflow-plugins/              # Custom Airflow plugins (if any)
â”‚   â”œâ”€â”€ docker-compose.yml            # Airflow services orchestration
â”‚   â”œâ”€â”€ Dockerfile                    # Custom Airflow Docker image
â”‚   â””â”€â”€ requirements.txt              # Python dependencies for Airflow
â”‚
â”œâ”€â”€ Datasets/                         # Source Data (Raw Datasets)
â”‚   â”œâ”€â”€ ca_crop_master.csv            # Crop reference & master data
â”‚   â”œâ”€â”€ ca_crop_production.csv        # Crop-wise production data
â”‚   â”œâ”€â”€ ca_fertilizer_usage.csv       # Fertilizer usage metrics
â”‚   â”œâ”€â”€ ca_rainfall_data.csv          # Rainfall & climate data
â”‚   â””â”€â”€ ca_soil_health.csv            # Soil health indicators
â”‚
â”œâ”€â”€ Capstone_Chubb_Databricks/        # Databricks Lakehouse
â”‚   â””â”€â”€ Capstone_Chubb/
â”‚       â”œâ”€â”€ bronze/                   # Raw ingestion tables (Bronze layer)
â”‚       â”œâ”€â”€ silver/                   # Cleaned & validated tables (Silver layer)
â”‚       â”œâ”€â”€ gold/                     # Aggregated analytics tables (Gold layer)
â”‚       â””â”€â”€ Agricultural_Logging/     # Pipeline logging & monitoring
â”‚
â”œâ”€â”€ dashboard/                        # Business Intelligence Layer
â”‚   â””â”€â”€ agricultural_analytics.pbix   # Power BI dashboard
â”‚
â”œâ”€â”€ images/                           # Documentation images
â”‚   â””â”€â”€ system_architecture.png       # System architecture diagram
â”‚
â””â”€â”€ README.md                         # Project documentation
```
---
## ğŸ§± Data Layers (Medallion Architecture)

The project follows the **Bronzeâ€“Silverâ€“Gold lakehouse architecture** implemented on Databricks using Delta Lake, ensuring scalability, reliability, and analytics readiness.

---

## ğŸ¥‰ Bronze Layer â€“ Raw Data Foundation

The Bronze layer stores **raw, immutable agricultural datasets** ingested directly from source files with minimal transformation.

### Purpose
- Preserve original source data
- Enable traceability and auditability
- Support reprocessing if required

### Bronze Tables

| Table Name | Description |
|----------|-------------|
| `bronze_crop_master` | Raw crop master reference data |
| `bronze_crop_production` | Raw crop-wise production data |
| `bronze_fertilizer_usage` | Raw fertilizer usage data |
| `bronze_rainfall` | Raw rainfall and climate data |
| `bronze_soil_health` | Raw soil health indicators |

### Key Characteristics
- Immutable Delta tables  
- Schema enforcement enabled  
- Ingestion timestamps captured  
- Source lineage maintained  


## ğŸ¥ˆ Silver Layer â€“ Cleaned & Curated Data

The Silver layer contains **validated, standardized, and analytics-ready datasets** derived from Bronze tables.

### Purpose
- Improve data quality
- Standardize business dimensions
- Filter invalid or inconsistent records

### Silver Tables

| Table Name | Description |
|-----------|------------|
| `silver_crop_master` | Cleaned and standardized crop master reference data |
| `silver_crop_production` | Cleaned and validated crop production data |
| `silver_fertilizer_usage` | Cleaned and validated fertilizer usage data |
| `silver_rainfall` | Cleaned rainfall data |
| `silver_soil_health` | Cleaned soil health data |
|

### Reject & Quarantine Tables
Records failing validation rules are isolated for audit and debugging.

| Table Name | Description |
|-----------|------------|
| `silver_reject_crop_production` | Rejected crop production records |
| `silver_reject_fertilizer_usage` | Rejected fertilizer usage records |
| `silver_reject_rainfall` | Rejected rainfall records |
| `silver_reject_soil_health` | Rejected soil health records |
| `quarantine_crop_production` | Quarantined records for reprocessing |

### Key Characteristics
- Business rule enforcement  
- Referential integrity checks  
- Numeric range and null validations  
- High-quality analytics-ready schema  


## ğŸ¥‡ Gold Layer â€“ Business Intelligence & Analytics

The Gold layer provides **aggregated, KPI-driven datasets** optimized for Power BI reporting and decision-making.

### Purpose
- Enable fast BI queries
- Provide single source of truth
- Support executive dashboards

### Gold Tables

| Table Name | Description |
|-----------|------------|
| `gold_crop_yield_summary` | Yield metrics aggregated by crop, region, and year |
| `gold_fertilizer_efficiency` | Fertilizer usage vs yield efficiency analysis |
| `gold_region_performance` | Regional production and yield performance metrics |

### Key Characteristics
- Pre-aggregated for performance  
- Denormalized for BI simplicity  
- Optimized for Power BI consumption  
- Consistent KPI definitions  

---

## ğŸ“œ Pipeline Logging & Monitoring

### Logging Tables

| Table Name | Description |
|-----------|------------|
| `pipeline_logs` | End-to-end ETL execution logs |

### Logging Capabilities
- Pipeline execution timestamps  
- Success / failure status tracking  
- Error diagnostics  
- Operational monitoring support  


### âœ… Summary
- Bronze ensures **data integrity**
- Silver ensures **data quality**
- Gold ensures **business value**
- Reject & log tables ensure **observability and reliability**
---
## ğŸ”„ Orchestration with Apache Airflow

Apache Airflow is used as the **central workflow orchestrator** to manage and monitor the end-to-end ETL pipeline across the **Bronze, Silver, and Gold layers**.

The orchestration layer ensures:
- Reliable scheduling of ETL jobs
- Dependency management between data layers
- Operational visibility and monitoring
- Automated retries and failure handling


### ğŸ§© Airflow DAG Design

The pipeline is implemented as a **task-based DAG**, where each task represents a logical stage of the data pipeline:

| Task Name | Description |
|---------|-------------|
| `bronze_job` | Ingest raw agricultural datasets into Bronze Delta tables |
| `silver_job` | Clean, validate, and standardize data into Silver tables |
| `gold_task` | Generate aggregated analytics and KPIs in Gold tables |

**Task Dependency Flow:**

Each task is triggered only after the successful completion of its upstream dependency.

---

### ğŸ“¸ Pipeline Execution Monitoring

<table>
  <tr>
    <td align="center">
      <img src="images/Databricks_Job.png" width="450"/>
      <br>
      <em>Airflow-Orchestrated Databricks Job Runs</em>
    </td>
    <td align="center">
      <img src="images/Airflow_call.png" width="450"/>
      <br>
      <em>Apache Airflow DAG â€“ Bronze â†’ Silver â†’ Gold</em>
    </td>
  </tr>
</table>
---

### âš™ï¸ Orchestration Features

- â±ï¸ **Scheduled Execution** â€“ Automated pipeline runs based on defined schedules  
- ğŸ”— **Dependency Management** â€“ Strict Bronze â†’ Silver â†’ Gold execution order  
- ğŸ”„ **Retry Mechanism** â€“ Automatic retries on transient failures  
- ğŸ“Š **Execution Monitoring** â€“ Task duration, success, and failure tracking  
- ğŸ§¾ **Audit Logging** â€“ Execution details captured in pipeline log tables  

---

## ğŸ“Š Power BI Analytics Suite

The Power BI layer delivers **interactive, executive-ready dashboards** built on curated **Gold-layer Delta tables**, enabling data-driven agricultural decision-making.


### ğŸ“‘ Report Pages Overview

The Power BI report is organized into multiple analytical views, each addressing a specific business question:

| Report Page | Description | Business Value |
|------------|-------------|----------------|
| ğŸ“Œ **Executive Overview** | High-level KPIs including total production, average yield, and regional performance | Strategic planning & monitoring |
| ğŸŒ§ï¸ **Rainfall-Driven Yield Analysis** | Analysis of rainfall impact on crop yield across seasons and regions | Climate impact assessment |
| ğŸ—ºï¸ **Regional Performance** | State- and district-level production and yield comparison | Regional optimization |
| ğŸŒ¾ **Agricultural Yield Drivers Analysis** | Yield drivers such as fertilizer usage, soil health, and rainfall | Yield improvement insights |


### âš™ï¸ Dashboard Capabilities

- ğŸ›ï¸ **Interactive Slicers** â€“ Year, State, District, Crop, and Season  
- ğŸ“ˆ **KPI Cards** â€“ Production, Yield, Growth indicators  
- ğŸ” **Drill-Down Analysis** â€“ State â†’ District â†’ Crop level insights  
- ğŸ”„ **Automated Refresh** â€“ Synced with Gold-layer Delta tables  
- ğŸ“¤ **Export Options** â€“ PDF, Excel, and PowerPoint  

### ğŸ¯ Business Impact
- Enables identification of **high- and low-performing regions**
- Improves visibility into **yield-influencing factors**
- Supports **data-driven agricultural policy and planning**
- Reduces manual analysis and reporting effort
---

## ğŸš€ Quick Start

This section provides step-by-step instructions to set up and run the **Agricultural Crop Production & Yield Optimization Analytics System**, covering environment setup, pipeline execution, and analytics consumption.

---

### âœ… Prerequisites

Before starting, ensure the following are available:

- Docker Desktop (latest stable version)
- Docker Compose v2 or higher
- Python 3.8+
- Access to a Databricks workspace
- Databricks personal access token
- Power BI Desktop
- Git
- Minimum 8 GB RAM (16 GB recommended)

### ğŸ“¥ Step 1: Clone the Repository

Clone the project repository and navigate to the project directory.

```bash
git clone <your-github-repository-url>
cd Agricultural_Crop_Production_And_Yield_Optimization_Analytics_System
```


### ğŸ³ Step 2: Start Apache Airflow Services

Navigate to the Airflow directory and start all required services using Docker Compose.

```bash
cd capstone_airflow
docker compose up -d
```


### ğŸŒ Step 3: Access the Airflow Web Interface

Open the Airflow UI in your browser to monitor and manage pipelines.

```text
http://localhost:8080
```

Login credentials:

```text
Username: admin
Password: admin
```


### ğŸ”— Step 4: Configure Databricks Connection in Airflow

Configure Airflow to securely connect to Databricks for job execution.

```text
Airflow UI â†’ Admin â†’ Connections â†’ Add New
Connection ID   : databricks_default
Connection Type : Databricks
Host            : <Databricks Workspace URL>
Token           : <Databricks Personal Access Token>
```


### â–¶ï¸ Step 5: Execute ETL Pipelines

Trigger the ETL pipeline to process data across Bronze, Silver, and Gold layers.

```text
Airflow UI â†’ DAGs â†’ Enable ETL_DAG â†’ Trigger DAG
```

Pipeline execution order:

```text
Bronze Layer â†’ Silver Layer â†’ Gold Layer
```


### ğŸ“Š Step 6: Open Power BI Dashboard

Open the Power BI report and refresh it to view the latest analytics.

```text
dashboard/agricultural_analytics.pbix
```

Steps inside Power BI:

```text
Home â†’ Transform Data â†’ Data Source Settings â†’ Update Databricks Connection
Home â†’ Refresh
```


### âœ… Step 7: Verify Data & Logs

Validate successful execution by checking logs and output tables.

```text
Databricks â†’ Gold Tables
Databricks â†’ Agricultural_Logging Tables
Airflow UI â†’ Task Logs
```
---
## ğŸ¦– Troubleshooting

### Common Issues & Solutions

â–¶ ğŸš« **Airflow containers wonâ€™t start**  
- Ensure Docker Desktop is running  
- Verify Docker Compose version (`docker compose version`)  
- Check port `8080` is not already in use  
- Restart services using:
  ```bash
  docker compose down
  docker compose up -d
  ```

â–¶ ğŸ”‘ **Databricks connection fails**  
- Verify Databricks workspace URL  
- Ensure the personal access token is valid and not expired  
- Confirm the Airflow connection ID matches the DAG configuration  

â–¶ ğŸ“Š **Power BI data refresh issues**  
- Validate Databricks SQL endpoint configuration  
- Re-check catalog and schema names  
- Ensure the Gold tables exist and are accessible  

â–¶ âš ï¸ **Pipeline execution failures**  
- Review task logs in Airflow UI  
- Check Databricks job run logs  
- Inspect reject and quarantine tables for invalid records  

---

## ğŸ“š Documentation

### Additional Resources

- ğŸ“˜ [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- ğŸ“˜ [Databricks Best Practices](https://docs.databricks.com/)
- ğŸ“˜ [Delta Lake Guide](https://delta.io/)
- ğŸ“˜ [Power BI Documentation](https://learn.microsoft.com/power-bi/)
- ğŸ“˜ [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)

---

### Project Artifacts

- ğŸ“Š **Sample Dataset** â€“ Raw agricultural input data  
- ğŸ““ **Jupyter Notebooks** â€“ Bronze, Silver, and Gold layer development  
- ğŸ¨ **Power BI Dashboard** â€“ Interactive analytics and KPIs  
- ğŸ“ˆ **Analytics Preview** â€“ Sample outputs from Gold tables
---

## ğŸ”® Future Enhancements

- Add real-time ingestion using Kafka or Auto Loader  
- Implement CI/CD for Airflow DAGs and Databricks jobs  
- Enable automated Power BI refresh using gateways  
- Introduce anomaly detection for yield and production  
- Add role-based access control for analytics  

---

## ğŸ Conclusion

This project demonstrates the successful design and implementation of an **end-to-end Agricultural Crop Production & Yield Optimization Analytics System** using modern data engineering and analytics tools.

By combining **Apache Airflow**, **Databricks**, **Delta Lake**, and **Power BI**, the solution enables scalable data processing, reliable ETL orchestration, and actionable insights for agricultural decision-making.

The project showcases real-world data engineering practices including **lakehouse architecture**, **data quality enforcement**, **workflow automation**, and **business intelligence reporting**, making it suitable for enterprise and production use cases.

---

## ğŸ“œ License

This project is licensed under the **MIT License**.  
You are free to use, modify, and distribute this project with proper attribution.

---

## ğŸ™ Acknowledgements

- Apache Airflow community  
- Databricks documentation and learning resources  
- Delta Lake open-source contributors  
- Microsoft Power BI documentation  
- PySpark and Apache Spark community  

---

## ğŸ‘¤ Author

**Bandaru Venkata Kaushik**  
ğŸ“§ Email: kaushizzbv@gmail.com  

ğŸ“Œ *Aspiring Data Engineer | Data Analytics & Lakehouse Architecture Enthusiast*  
ğŸ“Œ *Focused on building scalable, production-ready data engineering solutions*
---
